# Reading Materials
[**Beyond Physical Memory: Mechanisms**](https://pages.cs.wisc.edu/~remzi/Classes/537/Spring2018/Book/vm-beyondphys.pdf)
[**Beyond Physical Memory: Policies**](https://pages.cs.wisc.edu/~remzi/Classes/537/Spring2018/Book/vm-beyondphys-policy.pdf)

----
# Beyond Physical Memory: Mechanisms
- So far, we've assumed that all address spaces fit into physical memory, but in reality, modern systems need to support **large address spaces** for each process and handle **multiple concurrently-running processes**.
- This requires a way to extend beyond the limitations of physical memory, which leads to the concept of **virtual memory**.
### Virtual Memory
- Virtual memory allows each process to **act as if it has access to a large, contiguous block of memory**, even if not all of it fits in physical memory at once.
- The OS creates this illusion by using **swap space** on slower storage devices (e.g., hard drives) to store portions of a process’s memory that aren’t currently in active use.
- **Key advantage:** Programmers don’t have to manually manage memory constraints; they can allocate memory freely without worrying about physical memory limits.
### Memory Hierarchy
- The memory hierarchy consists of:
	- **Physical memory (RAM)**: Fast but limited in size.
	- **Disk (swap space)**: Larger but much slower.
- Virtual memory uses this hierarchy to **extend the address space** by moving less-used data to swap space, keeping frequently used data in faster physical memory.
### Historical Context
- Early systems required programmers to manually handle memory with **overlays**, moving data and code in and out of memory themselves.
- Virtual memory eliminates this need by **automatically managing memory** on behalf of the programmer.
- Without virtual memory, it would be impossible to run many large processes at once due to limited physical memory.
- Virtual memory enables multiprogramming by allowing the OS to manage memory dynamically, swapping pages between physical memory and disk as needed.
-----
## 21.1 - Swap Space
- **Swap space** is disk space reserved for moving pages between physical memory and disk.
- It allows the OS to **swap out** pages from memory to disk and **swap in** pages back into memory when needed.
- The swap space serves as an extension of physical memory, helping the system **manage larger address spaces** than the available RAM would otherwise allow.
### Importance of Swap Space
- The **size of the swap space** determines the maximum number of memory pages the system can handle.
- In real-world scenarios, swap space must be large enough to support the needs of all active processes but is assumed to be large enough for simplicity in initial examples.
### Example: Swap Space and Processes
![[Figure-21.1-SwapSpace.png]]
- In a scenario with 4 pages of physical memory and 8 pages of swap space:
	- **Three processes** (Proc 0, Proc 1, Proc 2) have some of their pages in memory, while the rest reside in the swap space.
	- A **fourth process** (Proc 3) has all its pages swapped out and is not actively running.
### Code Pages and Swap Traffic
- Not all swap traffic comes from swap space. **Program binaries**, such as `ls` or other executables, initially reside on disk.
- When a program runs, **code pages are loaded from disk** into memory, either all at once or **on-demand** (as modern systems do).
- If the system needs more memory, it can safely re-use memory that held code pages, since it can reload those pages from the **on-disk binary** in the file system later.
### Advantages of Swap Space
- **Simulates larger memory** than the actual physical memory (RAM).
- Supports running **multiple processes** efficiently by offloading unused memory pages to disk.
- Enables **code reloading** from binaries, avoiding the need to permanently store all code in physical memory.
-----
## 21.2 - The Present Bit
- A process generates **virtual memory references** (for instructions or data).
- The **hardware translates** these virtual addresses into physical addresses using the **Translation Lookaside Buffer (TLB)**.
- **TLB hit**: If the VPN (Virtual Page Number) is found in the TLB, the physical address is fetched directly from memory.
- **TLB miss**: If the VPN is not in the TLB, the hardware looks up the page in the **page table** (using the VPN) to find the corresponding **Physical Frame Number (PFN)**.
### Present Bit
- When supporting **swapping pages to disk**, each **Page Table Entry (PTE)** includes a **present bit**.
- The present bit indicates whether the page is currently in **physical memory**:
	- **Present bit = 1**: The page is in memory, and normal memory access continues.
	- **Present bit = 0**: The page is not in memory, but stored on **disk**.
### Page Fault
- If the present bit is **0**, the system encounters a **page fault**.
- A **page fault** occurs when the process tries to access a page that is not in physical memory but resides in the swap space on disk.
- This triggers the **OS page-fault handler**, which is responsible for retrieving the page from disk.
### Page-Fault Handler
- The **page-fault handler** is a special part of the OS that services page faults.
- It retrieves the required page from **swap space** and places it into physical memory.
- Once the page is in memory, the page table and TLB are updated to reflect the new mapping, and the process resumes execution.
-----
## 21.3 - The Page Fault
### Remember Management of TLBs
- **Hardware-Managed TLBs**: Hardware looks in the page table to find the translation for a TLB miss.
- **Software-Managed TLBs**: The OS handles the TLB miss by searching the page table and updating the TLB.
- Regardless of TLB management, **page faults** are always handled by the **OS**.
### Disk Address in Page Table Entry (PTE)
- The OS stores the **disk address** of swapped-out pages in the **PTE**.
- When a page fault occurs, the OS looks up the disk address in the PTE, retrieves the page from the disk, and loads it back into **physical memory**.
### Updating Page Table and TLB
- After the page is loaded into memory, the OS updates the **PTE**:
	- Marks the page as **present** in memory.
	- Updates the **Physical Frame Number (PFN)** in the PTE to reflect the page's location in memory.
- The TLB may also need to be updated with the new translation.
### Process Blocking and Multiprogramming
- While waiting for the **disk I/O** to complete (fetching the page), the process that caused the page fault enters a **blocked state**.
- The OS can schedule and run other processes while the I/O operation is in progress, utilizing **multiprogramming** to maximize CPU efficiency.
---
## 21.4 - What If Memory Is Full?
### Page Replacement Policy
- When **memory is full**, the OS may need to **swap out** one or more pages to free space for the incoming page.
- The **page-replacement policy** determines which page(s) to remove from memory to make room for new pages.
- The goal is to select a page that minimizes the performance cost of replacing it.
### Importance of a Good Replacement Policy
- Choosing the **wrong page** to replace can significantly impact performance.
- A poor decision can cause the program to run at **disk-like speeds** instead of **memory-like speeds**, potentially making it **10,000 to 100,000 times slower**.
### Need for Efficient Replacement
- A well-designed replacement policy ensures optimal use of physical memory and avoids excessive **disk I/O**.
- Efficient page replacement is **crucial** for maintaining overall system performance.
-----
## 21.5 - Page Fault Control Flow
- When a program fetches data from memory, a sequence of events occurs depending on whether the data is present in memory or needs to be retrieved from disk.
### Hardware Control Flow
- **TLB Miss:** When a TLB miss occurs, there are three possible scenarios:
	1. **Valid and Present Page (TLB miss only):**
		- The page is present in physical memory but not in the TLB. The TLB miss handler fetches the **PFN** from the page table, updates the TLB, and retries the instruction, resulting in a **TLB hit**.
	2. **Valid but Not Present Page (Page Fault):**
		- The page is valid but has been **swapped to disk**. The OS needs to handle the **page fault**, read the page back into memory, and update the page table.
	3. **Invalid Page:**
		- If the page is invalid (e.g., due to a bug in the program), the OS traps the invalid access and likely terminates the process.
### Software Control Flow (Page Fault Handling)
- **Page Fault Handler:**
	1. The OS must find a **free physical frame** for the page. If no free frames are available, the **page-replacement algorithm** runs to free up space by swapping out another page.
	2. The handler issues an **I/O request** to load the page from **swap space** into memory.
	3. Once the I/O completes, the OS updates the **page table** and retries the instruction.
	4. This may result in a **TLB miss** again, which is then serviced. Finally, after retrying, the desired data is fetched, and the program continues.
-----
## 21.6 - When Replacements Really Occur
### High Watermark (HW) and Low Watermark (LW)
- Operating systems aim to **proactively manage memory** by keeping a portion of it free rather than waiting until memory is completely full.
- The system defines two thresholds:
	- **Low Watermark (LW):** When free pages fall below this level, the OS begins freeing pages.
	- **High Watermark (HW):** The OS evicts pages until the number of free pages reaches this level.
### Background Paging Thread
- **Swap Daemon/Page Daemon:** A **background thread** that runs when free pages are low (below LW). It **evicts pages** from memory to ensure there are enough free pages (up to HW).
- Once enough pages have been freed, the daemon goes to sleep until it is needed again.
### For Efficiency
- When evicting pages, the OS often **groups pages** together and writes them to disk in bulk (called **clustering**), improving disk performance by reducing seek and rotational delays.
- When a process needs memory and finds no free pages, it informs the **background paging thread** to free pages. The process then waits until pages are available before continuing.
-----
# Beyond Physical Memory: Policies
- **THE CRUX: HOW TO DECIDE WHICH PAGE TO EVICT**
    - How can the OS decide which page (or pages) to evict from memory? This decision is made by the replacement policy of the system, which usually follows some general principles but also includes certain tweaks to avoid corner-case behaviors.
---
## 22.1 - Cache Management
### The Problem
- **Main memory** acts as a **cache** for virtual memory pages.
- **Goal:** Minimize **cache misses** (when a page is not in memory and must be fetched from disk) or maximize **cache hits** (when a page is found in memory).
- **Average Memory Access Time (AMAT)**: This is the average time it takes to access memory, including both cache hits and misses.
### AMAT Formula
$$AMAT = TM + (PMiss × TD)$$ - **TM**: Time to access memory. - **PMiss**: Probability of missing (miss rate). - **TD**: Time to access disk.
- **Example**: With a 90% hit rate and a miss rate of 10% (0.1), where TM = 100 ns and TD = 10 ms:
    - **AMAT** = 100 ns + (0.1 × 10 ms) = 1 ms.
### Understanding the Impact of Misses
- Even a **tiny miss rate** can drastically increase the AMAT due to the large difference between memory access time and disk access time.
- In the example, increasing the hit rate to 99.9% improves AMAT significantly to **10.1 microseconds**.
### Significance of a Good Replacement Policy
- As **disk access** is **much slower** than memory, it is crucial to **reduce misses** as much as possible.
- A **smart replacement policy** helps minimize the number of misses, thereby improving overall system performance.
---
## 22.2 - The Optimal Replacement Policy
- Developed by **Belady** in 1966, this policy is called **MIN** and is an **optimal page replacement** policy.
- The policy replaces the page that will be accessed **furthest in the future**, minimizing cache misses.
### Intuition Behind MIN
- **Goal**: Minimize future misses by evicting the page that will not be used for the longest time.
- If you have to replace a page, removing the one needed **farthest in the future** ensures that all other pages in the cache will be used sooner.
- **Example of MIN:**
    - Consider the following **sequence of page accesses**: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1.
    - Assume the cache can hold **3 pages**.
    - The first three accesses are **misses** (cold-start or compulsory misses).
    - When a page replacement is needed (e.g., on accessing page 3), MIN examines which pages (0, 1, 2) will be used furthest in the future and **evicts page 2**.
    - The sequence of actions in the example results in **6 hits** and **5 misses**, with a **hit rate of 54.5%**.
### Hit Rate Calculations
- **Hit Rate**: $Hits/(Hits+Misses)$
    - In the example: $6/(6+5)=54.5\%$     
- **Adjusted Hit Rate (excluding compulsory misses)**: $85.7\%$
### Limitations of MIN
- The **future access pattern** is not generally known in practice.
- **Optimal policy** (MIN) is useful only as a **benchmark** to compare how close other real-world page replacement policies come to being perfect.
---
## 22.3 - A Simple Policy: FIFO
### FIFO Replacement Policy
- **First-In, First-Out (FIFO)** is a simple replacement policy where pages are managed in a **queue**.
- Pages are added to the **tail** of the queue when they enter the system, and when a replacement is needed, the page at the **head** (the oldest one) is evicted.
### Strength of FIFO
- The **primary advantage** of FIFO is its **simplicity** in implementation.
- There is no need for complex tracking of access patterns—just keep pages in order of arrival
- **Example of FIFO with Reference Stream:**
    - Assume the following **page reference stream**: 0, 1, 2, 0, 1, 3, 0, 3, 1, 2, 1.
    - In the example, we begin with **three compulsory misses** for pages 0, 1, and 2.
    - After **hits** on pages 0 and 1, page 3 causes a **miss**, and FIFO evicts page 0 (the "first-in" page), despite page 0 being accessed again shortly afterward, resulting in another miss.
    - This leads to **multiple unnecessary misses** because FIFO doesn’t account for the **importance** of frequently accessed pages.
### Comparison to Optimal (MIN)
- FIFO performs **worse than the optimal policy (MIN)**.
- **Hit rate**: 36.4% overall, and **57.1% excluding compulsory misses**.
- **Key limitation**: FIFO **cannot differentiate between important and unimportant pages**—it evicts based on age, not usage.
- **Belady's Anomaly**
