#  Reading Materials
[**Scheduling: Introduction**](https://pages.cs.wisc.edu/~remzi/Classes/537/Spring2018/Book/cpu-sched.pdf)
[**Scheduling: The Multi-Level Feedback Queue**](https://pages.cs.wisc.edu/~remzi/Classes/537/Spring2018/Book/cpu-sched-mlfq.pdf)
[**Summary Dialogue on CPU Virtualization**](https://pages.cs.wisc.edu/~remzi/Classes/537/Spring2018/Book/cpu-dialogue.pdf)

----
# Scheduling: Introduction
- Scheduling in computer systems has roots in operations management, as both fields aim to efficiently manage tasks or processes.
- The challenge is developing an effective **scheduling policy** that optimizes CPU usage. Key considerations include:
	- **Assumptions:** What are the conditions or constraints in the system?
	- **Metrics:** What criteria (e.g., efficiency, fairness, response time) should the policy optimize?
	- **Approaches:** What strategies have been historically used in early computer systems to guide scheduling decisions?
-----
## 7.1 - Workload Assumptions
- To develop effective scheduling policies, we need to define the **workload**—the characteristics of processes (jobs) running in the system.
- These assumptions simplify the initial exploration of scheduling policies, though they may not reflect real-world scenarios.
### Simplifying Assumptions
- **Assumption 1: Equal Job Run Times**: All jobs in the system are assumed to run for the same amount of time.
- **Assumption 2: Simultaneous Job Arrival**: All jobs are assumed to arrive at the system at the same time.
- **Assumption 3: No Preemption (Run to Completion)**: Once a job starts, it runs to completion without interruption or preemption.
- **Assumption 4: CPU-bound Jobs**: Jobs are assumed to use only the CPU and perform no I/O operations.
- **Assumption 5: Known Job Run Times**: The run-time of each job is known in advance, giving the scheduler complete knowledge of how long each job will take.
### Unrealistic Assumptions
- While these assumptions simplify policy design, they are not realistic in real-world systems.
    - **Example:** Knowing the exact run-time of each job is not feasible, as it would require the scheduler to be omniscient.
- As scheduling policies evolve, these assumptions will be relaxed to better reflect the complexities of real workloads.
- The goal is to eventually develop a **fully-operational scheduling discipline** that can handle more realistic scenarios.
-----
## 7.2 - Scheduling Metrics
- Metrics allow us to evaluate and compare different scheduling policies by providing a way to measure performance.
### Turnaround Time
- **Turnaround time** is the total time a job spends in the system, calculated as the time a job completes minus the time it arrived.
$$T_{turnaround} = T_{completion} − T_{arrival}$$
- **Simplified Case:** We assume all jobs arrive at the same time (Tarrival = 0), the formula simplifies to: $T_{turnaround} = T_{completion}$
- **Primary Metric:** Turnaround time is a **performance metric** that helps us evaluate how quickly jobs complete in a given scheduling policy.
- **Fairness Metric:**
    - Another important metric in scheduling is **fairness**, which ensures that all jobs get a chance to run without significant delay.
    - One example of measuring fairness is **Jain’s Fairness Index**.
- **Trade-off Between Performance and Fairness:** Scheduling policies often have to balance between **performance** and **fairness**.
	- A policy optimized for performance might favor some jobs, leaving others delayed, which reduces fairness.
----
## 7.3 - First in, First Out (FIFO)
- **FIFO (First In, First Out)** or **FCFS (First Come, First Served)** is a basic scheduling algorithm where jobs are executed in the order they arrive.
	- **Simplicity:** Easy to implement and understand.
	- **Order of Execution:** Jobs are processed in the order they enter the system.
### FIFO Scheduling with Equal Job Times
- Three jobs (A, B, and C) arrive at the same time (Tarrival = 0), each running for 10.
    - **Completion Times:**
        - A finishes at 10, B at 20, and C at 30.
    - **Average Turnaround Time:** $T_{turnaround} = (10 + 20 + 30) / 3 = 20$
### FIFO Performance with Varying Job Lengths
- When jobs have different lengths, FIFO can perform poorly.
- A job (A) runs for 100, while jobs B and C run for 10 each.
	- A runs for the full 100, causing B and C to wait in line.
	- **Average Turnaround Time:** $T_{turnaround} = (100 + 110 + 120) / 3 = 110$
- **Problem:**
	- This creates the **convoy effect**, where shorter jobs are delayed by a long-running job, leading to high average turnaround times and poor system performance.
### Drawbacks
- FIFO works well when all jobs have similar execution times.
- It performs poorly when jobs have varying lengths, as longer jobs block shorter jobs from executing.
- **Challenge:** How can we improve scheduling algorithms to handle jobs with different lengths more efficiently and avoid the convoy effect?
-----
## 7.4 - Shortest Job First (SJF)
- **SJF (Shortest Job First)** is a scheduling policy where the shortest job is executed first, followed by the next shortest, and so on.
    - **Goal:** To minimize average turnaround time by prioritizing shorter jobs over longer ones.
### SJF Scheduling to Improve over FIFO
- Three jobs (A, B, and C) arrive at the same time, but A runs for 100, while B and C run for 10 each.
- **SJF Schedule:** Instead of running A first, SJF runs B and C first, significantly reducing the average turnaround time.
- **Results:**
	- Turnaround time with FIFO: 110
	- Turnaround time with SJF: $T_{turnaround} = (10 + 20 + 120) / 3 = 50$
### SJF Performance
- **Optimality:**  Under the assumption that all jobs arrive at the same time, SJF is **optimal** for minimizing average turnaround time.
- **Improvement:** SJF often performs **much better** than FIFO, especially when job lengths vary.
### Challenges When Relaxing Assumptions
- In real-world systems, jobs do not arrive all at once but at different times.
- A arrives at t = 0 and runs for 100, while B and C arrive at t = 10 and each run for 10
- **SJF Problem:** Despite B and C arriving shortly after A, they are forced to wait until A finishes, leading to a **convoy problem** similar to FIFO.
	- **Turnaround Time:** (100 + (110 − 10) + (120 − 10)) / 3 = 103.33
### Limitations of SJF
- **Convoy Problem:** Even with SJF, longer jobs that arrive earlier can delay shorter jobs that arrive later, leading to inefficiencies and high turnaround times.
- **Challenge:** How can a scheduler adapt to jobs arriving at different times while maintaining efficiency? This requires further refinement of scheduling policies.
-----
## 7.5 - Shortest Time-to-Completion First (STCF)
- **STCF (Shortest Time-to-Completion First)**, also known as **Preemptive Shortest Job First (PSJF)**, allows the scheduler to preempt the current job and switch to a new job with the shortest remaining time to completion.
	- **Key Mechanism:** Whenever a new job arrives, the scheduler compares the remaining times of all jobs (including the new one) and runs the job with the least remaining time.
### Problem with Non-Preemptive SJF
- In the **non-preemptive** Shortest Job First (SJF) algorithm, jobs must run to completion once started, which can lead to inefficiencies when shorter jobs arrive after longer ones.
- **Solution:** To improve scheduling efficiency, **preemption** can be introduced to allow the scheduler to interrupt a long-running job when shorter jobs arrive.
### Using STCF to Improve over SJF
- Job A arrives at t = 0 and runs for 100, while jobs B and C arrive at t = 10 and each need to run for 10.
- **With STCF:** When B and C arrive, the scheduler **preempts** job A and runs B and C first. After B and C complete, the scheduler resumes job A.
- **Results:**
    - **Turnaround Time:**
        - Turnaround time with SJF: 103.33
        - Turnaround time with STCF: $T_{turnaround} = (120 − 0) + (20 − 10) + (30 − 10) / 3 = 50$
### Benefits of STCF
- **Optimality:** STCF is **optimal** in terms of minimizing turnaround time, just as SJF is optimal when all jobs arrive at the same time.
- **Preemption:** By allowing preemption, STCF avoids the **convoy effect** seen in non-preemptive schedulers like SJF and FIFO, where long-running jobs block shorter jobs.
- **Improved Efficiency:** STCF ensures that jobs with shorter remaining times are prioritized, improving overall system performance and reducing average turnaround times.
-----
## 7.6 - New Metric: Response Time
- **Response time** is the time from when a job arrives in the system to when it is first scheduled and begins executing.
$$T_{response} = T_{firstrun} − T_{arrival}$$
    - **Example (from Figure 7.5):** All jobs take 10
        - Job A: Arrive at t = 0 -- $0 - 0 = 0$
        - Job B: Arrive at t = 10 -- $10 - 10 = 0$
        - Job C: Arrive at t = 10 -- $20 - 10 = 10$
        - **Average Response Time:** (0 + 0 + 10) / 3 = 3.33
### Limitations of STCF for Response Time
- **Shortest Time-to-Completion First (STCF)** is optimized for **turnaround time** but can perform poorly in terms of response time.
    - **Problem:** When multiple jobs arrive at the same time, the later jobs may experience long delays before they are even scheduled, leading to poor response times.
    - **Example:** If three jobs arrive at once, the third job may have to wait for the first two to complete before it gets scheduled, resulting in a long response time.
- With the rise of interactive systems (e.g., users at terminals), **response time** became an important metric, as users expect quick feedback from the system.
- **User Experience:** A scheduling policy that results in long response times (e.g., waiting 10 for a response) can negatively impact the user experience, making the system feel unresponsive.
### New Challenge for Scheduling
- **Balancing Turnaround and Response Time:** While STCF is great for minimizing **turnaround time**, it is not ideal for **response time**, especially in interactive environments.
    - **Next Step:** The challenge is to design a scheduler that is **sensitive to response time**, providing quick feedback to users while balancing overall system efficiency.
-----
## 7.7 - Round Robin
- **Round-Robin (RR)** is a scheduling algorithm where each job is given a **time slice** (or quantum) to run before the next job in the queue is scheduled. This continues in a cycle until all jobs are complete.
- **Time Slice:** The length of the time slice must be a multiple of the timer interrupt (e.g., if the timer interrupts every 10 milliseconds, time slices could be 10 ms, 20 ms, etc.). RR is sometimes referred to as **time-slicing** due to its method of dividing CPU time equally among jobs.
### RR Scheduling
- Three jobs (A, B, C) arrive at the same time and each needs 5 seconds of CPU time.
- **SJF vs. RR:**
	- In SJF, jobs are run to completion one after the other.
	- In RR with a 1-second time slice, jobs are cycled through quickly.
- **Response Time Comparison:**
	- **RR Response Time:** (0 + 1 + 2) / 3 = 1 second (better for interactivity).
	- **SJF Response Time:** (0 + 5 + 10) / 3 = 5 seconds (worse for interactivity).
### The Importance of Time Slice Length
- **Shorter Time Slices:** Shorter time slices improve **response time** because jobs are cycled through more frequently.
- **Context-Switch Overhead:** If the time slice is too short, frequent context switching increases overhead, reducing overall performance.
	- **Context-Switch Costs:** Besides saving/restoring registers, there are hidden costs such as flushing CPU caches, TLBs, and branch predictors when switching between jobs.
### Round Robin Trade-offs
- **Turnaround Time Performance:** RR tends to perform poorly on **turnaround time** because it stretches out each job by only running them for a short period before switching to the next.
    - **Example:** A, B, and C (each needing 5 seconds) under RR: A finishes at 13, B at 14, C at 15, for an average turnaround time of 14 seconds.
    - **Conclusion:** RR often has worse turnaround time than even FIFO because it delays job completion by dividing CPU time equally across jobs.
- **Fairness:** RR is **fair** because it divides CPU time equally among all jobs on a small time scale.
    - **Trade-off:** Fairness improves response time but negatively impacts turnaround time.
		- Schedulers like SJF and STCF optimize for turnaround time but are bad for response time.
		- RR optimizes for response time but performs poorly on turnaround time.
-----
## KEY CPU VIRTUALIZATION TERMS (MECHANISMS)
- The CPU should support at least two modes of execution: a restricted user mode and a privileged (non-restricted) kernel mode.
- Typical user applications run in user mode, and use a system call to trap into the kernel to request operating system services.
- The trap instruction saves register state carefully, changes the hardware status to kernel mode, and jumps into the OS to a pre-specified destination: the trap table.
- When the OS finishes servicing a system call, it returns to the user program via another special return-from-trap instruction, which reduces privilege and returns control to the instruction after the trap that jumped into the OS.
- The trap tables must be set up by the OS at boot time, and make sure that they cannot be readily modified by user programs. All of this is part of the limited direct execution protocol which runs programs efficiently but without loss of OS control.
- Once a program is running, the OS must use hardware mechanisms to ensure the user program does not run forever, namely the timer interrupt. This approach is a non-cooperative approach to CPU scheduling.
- Sometimes the OS, during a timer interrupt or system call, might wish to switch from running the current process to a different one, a low-level technique known as a context switch.
----
# Scheduling: The Multi-Level Feedback Queue
- **MLFQ (Multi-Level Feedback Queue)** is a well-known scheduling algorithm that aims to balance the trade-offs between optimizing **turnaround time** and **response time**.
    - First developed by **Corbato et al.** in 1962 for the Compatible Time-Sharing System (CTSS), MLFQ has been refined and implemented in various modern systems.
- Like **Shortest Job First (SJF)** or **Shortest Time-to-Completion First (STCF)**, MLFQ aims to prioritize shorter jobs to improve turnaround time. However, the key challenge is that the OS **does not know** how long a job will run beforehand, which SJF/STCF requires.
- **Minimizing Response Time:** MLFQ also seeks to improve the system's responsiveness for **interactive users**, minimizing **response time**. While **Round-Robin (RR)** can reduce response time, it performs poorly for turnaround time, making it suboptimal.
- **Unknown Job Characteristics:** MLFQ must make scheduling decisions without prior knowledge of job characteristics (e.g., how long a job will run or how CPU-bound it is).
- **Dynamic Learning:** To overcome the lack of job knowledge, MLFQ needs to **learn** the behavior of jobs dynamically as they run. This allows the scheduler to make smarter decisions over time and balance the needs of short jobs, long jobs, and interactive processes.
----
## 8.1 - MLFQ: Basic Rules
###  Structure of MLFQ
- **Multiple Queues:** MLFQ consists of several **distinct queues**, each with a different **priority level**. Jobs are assigned to these queues based on their behavior, and their position can change over time.
- **Priority Scheduling:**
    - **Rule 1:** If a job has a higher priority than another, it will be scheduled to run first.
    - **Rule 2:** If two jobs have the same priority, **Round-Robin (RR)** scheduling is used to alternate between them.
### Dynamic Priority
- **Key Feature:** The scheduler **adjusts job priorities dynamically** based on the job's behavior.
	- **Interactive Jobs:** Jobs that relinquish the CPU frequently (e.g., waiting for user input) are considered **interactive** and maintain **high priority**.
	- **CPU-Intensive Jobs:** Jobs that use the CPU for long, uninterrupted periods are considered **CPU-bound** and their **priority is reduced** over time.
- This **adaptive learning** allows MLFQ to predict future behavior and allocate resources accordingly.
- **Snapshot of Queues:** Imagine two jobs (A and B) are at the **highest priority level**, while jobs C and D are at **lower levels**. MLFQ will alternate time slices between A and B, giving them priority over C and D.
    - **Issue:** Without priority adjustment, lower-priority jobs might never run, leading to **starvation** for C and D.
- **Dynamic Scheduling:** The core of MLFQ lies in **how priorities change over time** based on job behavior. The scheduler must balance **interactivity** and **CPU usage** to ensure fair and efficient scheduling.
---
## 8.2 - Attempt #1: How To Change Priority
### Dynamic Priority Adjustment in MLFQ
- **Job Allotment:** **Allotment** refers to the amount of CPU time a job can spend at a particular priority level before its priority is reduced. Initially, a job's **allotment** is set to a single **time slice** (i.e., one unit of CPU time).
- **MLFQ Rules for Priority Changes:**
    - **Rule 3:** When a job enters the system, it is placed at the **highest priority** (topmost queue).
    - **Rule 4a:** If a job uses up its **allotment** without relinquishing the CPU, its priority is reduced, and it moves down one queue.
    - **Rule 4b:** If a job gives up the CPU (e.g., for I/O) before using up its allotment, its priority is **maintained**.
### Examples of MLFQ in Action
- **Example 1: Single Long-Running Job**
    - A long-running job starts in the **highest priority queue** but is moved to lower priority queues as it uses its full time slices without releasing the CPU.
    - Eventually, the job reaches the **lowest priority queue** (Q0) and remains there for the rest of its execution.
- **Example 2: Long-Running vs. Short Job**
    - In a scenario with one long-running CPU-bound job (A) and one short-running interactive job (B):
        - Job B, arriving after A, is inserted into the highest queue and runs before A.
        - Since B finishes quickly, MLFQ approximates **Shortest Job First (SJF)** behavior for short jobs, allowing them to complete faster.
        - Once B finishes, job A resumes execution in the lower-priority queue.
- **Example 3: I/O-Bound Job**
    - In a scenario with an interactive job (B) that frequently performs I/O and a long-running job (A):
        - Job B repeatedly relinquishes the CPU after short bursts of CPU usage for I/O, and thus remains at a **high priority**.
        - MLFQ ensures that **interactive jobs** like B receive quick CPU access, improving system responsiveness.
### Issues with MLFQ
- **Problem 1: Starvation**
    - **Starvation** occurs when interactive jobs dominate CPU time, preventing long-running jobs from receiving CPU time.
    - Without a mechanism to ensure progress for low-priority jobs, long-running jobs may be indefinitely delayed or "starve."
- **Problem 2: Gaming the Scheduler**
    - **Gaming** refers to exploiting the MLFQ algorithm by intentionally relinquishing the CPU just before using the full allotment, allowing the job to stay in the **higher-priority queue**.
    - By issuing frequent I/O operations, a job could unfairly monopolize the CPU, gaining more CPU time than intended.
- **Problem 3: Changing Job Behavior**
    - Jobs may change behavior over time (e.g., switching from CPU-bound to I/O-bound).
    - MLFQ, as currently designed, may not adjust to this change, treating a previously CPU-bound job as low priority even when it becomes interactive.
----
## 8.3 - Attempt #2: The Priority Boost
### Problem: Starvation in MLFQ
- **Issue with CPU-bound Jobs:**
    - Without proper handling, **CPU-bound jobs** can get stuck in the lower-priority queues and **starve** if interactive jobs dominate the higher-priority queues.
- **Priority Boosting Mechanism:**
    - **Rule 5:** After a certain time period (S), **boost the priority of all jobs** by moving them to the **topmost queue**.
    - This guarantees that even long-running CPU-bound jobs will periodically receive CPU time.
- **Prevents Starvation:**
    - By moving all jobs to the top queue periodically, even low-priority CPU-bound jobs will get CPU access and won’t be starved.
    - Once in the top queue, jobs share CPU time with other high-priority jobs using Round-Robin (RR) scheduling.
- **Handles Changing Job Behavior:**
    - If a **CPU-bound job** becomes **interactive**, the periodic boost ensures that it is treated properly, receiving priority similar to other interactive jobs.
### Example of Priority Boosting
- **Without Priority Boosting:**
    - A long-running job is starved by two short-running interactive jobs, leading to unfair CPU allocation and poor progress for the long-running job.
- **With Priority Boosting (S = 100 ms):**
    - The long-running job receives a **boost every 100 ms** and is moved to the top priority, allowing it to make **progress periodically**, even in the presence of interactive jobs.
### Choosing the Time Period (S)
- **Voo-doo Constants:**
    - The time period **S** is a critical value, but difficult to set optimally.
        - **Too High:** Long-running jobs could still **starve** if the boost doesn’t happen often enough.
        - **Too Low:** Interactive jobs may not get a **fair share** of CPU time if boosting happens too frequently.
    - The optimal value for **S** often depends on system behavior and workload, and can be adjusted by the **system administrator** or increasingly through **automated methods** such as machine learning.
----
## 8.4 - Attempt #3: Better Accounting
- **Original Issue:** Under the original rules (4a and 4b), a process could **retain its priority** by relinquishing the CPU before its **allotment expires** (e.g., through I/O operations). This allowed processes to **game** the scheduler, gaining more CPU time than intended.
### Improved CPU Time Accounting
- **Better Tracking:** The scheduler must **track CPU usage** across both long bursts and small bursts. Instead of resetting the allotment every time a process performs I/O, the scheduler **accumulates the total CPU time used** at each priority level.
- **Revised Rule 4:** Once a job uses up its time allotment at a given priority level, its **priority is reduced**, regardless of whether it relinquishes the CPU frequently or not.
    - This prevents a process from **staying in the same priority queue** by performing I/O before its allotment runs out.
### Anti-Gaming in Action
- **Old Rules (4a & 4b):** A process could issue an I/O operation before the allotment expires, allowing it to remain at the **same priority level**, effectively monopolizing the CPU.
- **New Rule (Anti-Gaming Rule 4):** Under the new rule, a process moves down in priority **once it exhausts its total allotment**, regardless of how it splits its CPU usage. This ensures that no process can **dominate CPU time** by gaming the system.
- **Fair CPU Allocation:** The revised rule ensures **fairness** by preventing processes from exploiting I/O behavior to gain an unfair share of CPU time.
- **Consistent Behavior:** Whether a process uses its CPU time in **one long burst** or **many small bursts**, it will still be **demoted** once its allotment is fully used, ensuring that all processes move through the priority queues fairly.
----
## 8.5 - Tuning MLFQ And Other Issues
### Configuring MLFQ:
- A major challenge in MLFQ is determining **how to configure** the scheduler for optimal performance. Important parameters include:
	- **Number of queues**: How many levels of priority should exist.
	- **Time slice per queue**: How long should each job run at a given priority level.
	- **Allotment**: The total CPU time a job gets before being demoted.
	- **Priority boost frequency**: How often to boost all jobs to prevent starvation and account for changes in behavior.
- **Different Time Slices for Different Queues:**
    - High-priority queues often have **shorter time slices** (e.g., 10 ms or less) to quickly alternate between **interactive jobs**.
    - Low-priority queues, containing **long-running CPU-bound jobs**, use **longer time slices** (e.g., 100+ ms), as these jobs require more processing time and fewer interruptions.
### Solaris MLFQ (Time-Sharing Scheduling Class)
- **Configurable Scheduler:** The **Solaris MLFQ** is easy to configure and uses a table-based approach to control scheduling behavior.
    - The table specifies:
        - **Time slices** for each priority level.
        - **Priority boosts** for jobs (e.g., every 1 second).
    - Example: **60 queues** with time slices ranging from **20 ms** (for high-priority interactive jobs) to **hundreds of milliseconds** (for low-priority jobs).
### Alternative MLFQ Implementations
- **Mathematical-Based Schedulers:** Some systems, like **FreeBSD (version 4.3)**, use **formulas** instead of tables to calculate job priority.
        - **CPU usage**: A key factor in determining priority.
        - **Usage decay**: Older CPU usage "decays" over time, ensuring jobs get a **priority boost** without explicitly resetting their priority.
    - This approach differs from the **fixed rules** described in the chapter and offers more dynamic control over scheduling.
### Additional Features in Schedulers
- **Reserved Priority Levels for OS Work:** Some systems reserve the **highest priority levels** for **operating system processes**, ensuring that critical system tasks receive timely CPU access.
- **User Advice on Priority (Nice Command):**
    - In certain systems, users can manually adjust the priority of their processes using the **nice** command, which allows for a slight increase or decrease in job priority.
    - This provides some control over scheduling, especially for user-specific workloads.